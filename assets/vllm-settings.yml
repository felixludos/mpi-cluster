
# codes:
#   llama3: 3
#   llama4: 4
#   phi: 5
#   gemma: 2
#   qwen: 6
#   deepseek/other: 7
#   openai: 9


gpt20b: # confirmed 2025-09-19 13:04:19
  # model: openai/gpt-oss-20b
  model: "{hf_dir}/models--openai--gpt-oss-20b/snapshots/2e8f8052ee2aeee907f76e08c08b9fdde8677ca8"
  port: 8900
  dtype: bfloat16
  enable-auto-tool-choice: yes
  tool-call-parser: openai
  # async-scheduling: yes
  # max-model-len: 32768
  # enable-auto-tool-choice: yes
  resources: {cpu: 16, ram: 128, gpu: 1, gpu-ram: 75}

gpt120b: # unconfirmed
  # model: openai/gpt-oss-120b
  model: "{hf_dir}/models--openai--gpt-oss-120b/snapshots/eabf0c518da7584a2e7dab4ab272709785a72126"
  port: 8998
  dtype: bfloat16
  tensor-parallel-size: 4
  enable-auto-tool-choice: yes
  tool-call-parser: openai
  # async-scheduling: yes
  # enforce-eager: yes
  # quantization: modelopt
  resources: {cpu: 32, ram: 512, gpu: 4, gpu-ram: 75, gpu-name: H100-80}

gpt120b-2gpu: # confirmed 2025-09-20 12:10:35
  # model: openai/gpt-oss-120b
  model: "{hf_dir}/models--openai--gpt-oss-120b/snapshots/eabf0c518da7584a2e7dab4ab272709785a72126"
  port: 8998
  dtype: bfloat16
  tensor-parallel-size: 2
  enable-auto-tool-choice: yes
  tool-call-parser: openai
  # async-scheduling: yes
  # enforce-eager: yes
  # quantization: modelopt
  resources: {cpu: 32, ram: 512, gpu: 2, gpu-ram: 120, gpu-name: B200}


phi4: # confirmed
  # model: microsoft/Phi-4-reasoning
  model: "{hf_dir}/models--microsoft--Phi-4-reasoning/snapshots/57faa5302213dfe4155297bb19f66d2959d0b137"
  port: 8050
  dtype: bfloat16
  trust-remote-code: yes
  # enable-auto-tool-choice: yes
  # tool-call-parser: llama3_json
  # chat-template: "{vllm_dir}/examples/tool_chat_template_phi4_mini.jinja"
  enable-reasoning: yes
  reasoning-parser: deepseek_r1
  resources: {cpu: 16, ram: 64, gpu: 1, gpu-ram: 50}

nemo-r: # confirmed 2025-07-21 14:05:09
  model: nvidia/OpenReasoning-Nemotron-32B
  port: 8078
  max-model-len: 65536
  dtype: bfloat16
  resources: {cpu: 16, ram: 64, gpu: 1, gpu-ram: 80}

nemo-super: # confirmed 2025-07-23 13:39:41 (2025-07-23 13:45:28 missing tool template)
  # model: nvidia/Llama-3_3-Nemotron-Super-49B-v1
  model: "{hf_dir}/models--nvidia--Llama-3_3-Nemotron-Super-49B-v1/snapshots/b7432dd613fbd41afc32e12dffe5a08b0142a37c"
  port: 8770
  trust-remote-code: yes
  tensor-parallel-size: 2
  resources: {cpu: 32, ram: 256, gpu: 2, gpu-ram: 80}

nemo-super-4gpu: # confirmed 2025-07-21 17:15:42
  # model: nvidia/Llama-3_3-Nemotron-Super-49B-v1
  model: "{hf_dir}/models--nvidia--Llama-3_3-Nemotron-Super-49B-v1/snapshots/b7432dd613fbd41afc32e12dffe5a08b0142a37c"
  port: 8771
  trust-remote-code: yes
  tensor-parallel-size: 4
  resources: {cpu: 32, ram: 256, gpu: 4, gpu-ram: 75, gpu-name: H100-80}

nemo-ultra: # unconfirmed (2025-07-23 13:39:53 - fails due to KeyError: 'layers.68.mlp.down_proj.input_scale')
  # model: nvidia/Llama-3_1-Nemotron-Ultra-253B-v1-FP8
  model: "{hf_dir}/models--nvidia--Llama-3_1-Nemotron-Ultra-253B-v1-FP8/snapshots/e078e0aaf3c56376960a67167cec3676d4c40cb5"
  port: 8778
  trust-remote-code: yes
  tensor-parallel-size: 8
  # enforce-eager: yes
  # quantization: modelopt
  resources: {cpu: 32, ram: 512, gpu: 8, gpu-ram: 75, gpu-name: H100-80}


qwen8b: # stale
  model: Qwen/Qwen3-8B
  dtype: bfloat16
  port: 8006
  gpu-memory-utilization: 0.95
  max-model-len: 16384
  max-num-seqs: 1
  enable-reasoning: yes
  reasoning-parser: deepseek_r1
  resources: {cpu: 16, ram: 64, gpu: 1, gpu-ram: 20}

qwen32b: # confirmed 2025-07-21 14:05:09
  # model: Qwen/Qwen3-32B
  model: "{hf_dir}/models--Qwen--Qwen3-32B/snapshots/30b8421510892303dc5ddd6cd0ac90ca2053478d"
  port: 8600
  dtype: bfloat16
  enable-reasoning: yes
  reasoning-parser: deepseek_r1
  max-model-len: 32768 # max is 40960 (doesnt fit into 80GB)
  # enable-auto-tool-choice: yes
  # tool-call-parser: hermes
  resources: {cpu: 16, ram: 128, gpu: 1, gpu-ram: 75}

qwen235b-base: # confirmed 2025-07-21 14:05:09
  # model: Qwen/Qwen3-235B-A22B-FP8
  model: "{hf_dir}/models--Qwen--Qwen3-235B-A22B-FP8/snapshots/f49ca1a1be0fcb351b40a0d22ed4166c664e0f39"
  port: 8661
  # dtype: fp8
  gpu-memory-utilization: 0.5
  # max-model-len: 40960
  # max-num-seqs: 1
  tensor-parallel-size: 8
  enable-expert-parallel: yes
  enable-reasoning: yes
  reasoning-parser: deepseek_r1
  resources: {cpu: 32, ram: 512, gpu: 8, gpu-ram: 75, gpu-name: H100-80}

qwen235b: # confirmed 2025-10-06 11:43:57
  model: "{hf_dir}/models--Qwen--Qwen3-235B-A22B-Instruct-2507-FP8/snapshots/ba82a1060073fa0ecdc70d7b1922ec071f60cf3e"
  port: 8668
  gpu-memory-utilization: 0.5
  max-model-len: 65536
  tensor-parallel-size: 8
  enable-expert-parallel: yes
  enable-reasoning: yes
  reasoning-parser: deepseek_r1
  resources: {cpu: 32, ram: 512, gpu: 8, gpu-ram: 75, gpu-name: H100-80}

qwen235b-t: # unconfirmed
  model: "{hf_dir}/models--Qwen--Qwen3-235B-A22B-Thinking-2507-FP8/snapshots/c3ce9d7adab3b55a3a673acf6abb02a3310fd48b"
  port: 8666
  gpu-memory-utilization: 0.5
  max-model-len: 100000
  tensor-parallel-size: 8
  enable-expert-parallel: yes
  enable-reasoning: yes
  reasoning-parser: deepseek_r1
  resources: {cpu: 32, ram: 512, gpu: 8, gpu-ram: 75, gpu-name: H100-80}

qwen235b-t-4gpu: # confirmed 2025-10-06 15:00:13
  model: "{hf_dir}/models--Qwen--Qwen3-235B-A22B-Thinking-2507-FP8/snapshots/c3ce9d7adab3b55a3a673acf6abb02a3310fd48b"
  port: 8866
  gpu-memory-utilization: 0.5
  max-model-len: 100000
  tensor-parallel-size: 4
  enable-expert-parallel: yes
  enable-reasoning: yes
  reasoning-parser: deepseek_r1
  resources: {cpu: 32, ram: 512, gpu: 4, gpu-ram: 150, gpu-name: B200}

qwen-next: # unconfirmed
  model: "{hf_dir}/models--Qwen--Qwen3-Next-80B-A3B-Instruct/snapshots/9c7f2fbe84465e40164a94cc16cd30b6999b0cc7"
  port: 8680
  gpu-memory-utilization: 0.8
  tensor-parallel-size: 4
  # enable-expert-parallel: yes
  # enable-reasoning: yes
  # reasoning-parser: deepseek_r1
  served-model-name: qwen3-next
  resources: {cpu: 32, ram: 256, gpu: 4, gpu-ram: 75}

qwen-next-t: # unconfirmed
  model: "{hf_dir}/models--Qwen--Qwen3-Next-80B-A3B-Thinking/snapshots/e502dd4100cc68c0de57643fd4317ec93a128670"
  port: 8660
  gpu-memory-utilization: 0.8
  tensor-parallel-size: 4
  # enable-expert-parallel: yes
  # enable-reasoning: yes
  # reasoning-parser: deepseek_r1
  served-model-name: qwen3-next
  resources: {cpu: 32, ram: 256, gpu: 4, gpu-ram: 75}


# Qwen/Qwen3-235B-A22B-Thinking-2507-FP8

mistral7b: # stale
  model: mistralai/Mistral-7B-Instruct-v0.3
  dtype: bfloat16
  port: 8007
  # gpu-memory-utilization: 0.9
  max-model-len: 16384
  max-num-seqs: 1
  tokenizer-mode: mistral
  config-format: mistral
  load-format: mistral
  resources: {cpu: 16, ram: 64, gpu: 1, gpu-ram: 20}

mistral-small: # confirmed 2025-09-20 16:12:58
  # model: mistralai/Mistral-Small-3.2-24B-Instruct-2506
  model: "{hf_dir}/models--mistralai--Mistral-Small-3.2-24B-Instruct-2506/snapshots/46a27874d7f7a7b38344124d32a7a3c4589d3b53"
  dtype: bfloat16
  port: 8070
  tokenizer-mode: mistral
  config-format: mistral
  load-format: mistral
  tool-call-parser: mistral
  enable-auto-tool-choice: yes
  # limit-mm-per-prompt: 10
  resources: {cpu: 16, ram: 64, gpu: 1, gpu-ram: 50}

magistral: # confirmed 2025-09-20 16:38:33 
  # model: mistralai/Magistral-Small-2509
  model: "{hf_dir}/models--mistralai--Magistral-Small-2509/snapshots/92ad3595fd6f2c6299dadbc013460b313a29c063"
  dtype: bfloat16
  port: 8070
  tokenizer-mode: mistral
  config-format: mistral
  load-format: mistral
  tool-call-parser: mistral
  enable-auto-tool-choice: yes
  # limit-mm-per-prompt: 10
  resources: {cpu: 16, ram: 64, gpu: 1, gpu-ram: 50}


hunyuan: # confirmed 2025-07-21 14:05:09
  model: tencent/Hunyuan-A13B-Instruct
  dtype: bfloat16
  port: 8700
  # max-model-len: 65536
  trust-remote-code: yes
  tensor-parallel-size: 2
  resources: {cpu: 32, ram: 256, gpu: 2, gpu-ram: 75}

k2: # confirmed 2025-09-20 12:25:27
  # model: LLM360/K2-Think
  model: "{hf_dir}/models--LLM360--K2-Think/snapshots/64d16d025d16ed46b785fce99b2b548901b993d9"
  dtype: bfloat16
  port: 8080
  max-model-len: 48000
  resources: {cpu: 32, ram: 256, gpu: 1, gpu-ram: 75}

# seed oss


llama70b: # confirmed 2025-10-06 11:44:33
  # model: meta-llama/Llama-3.3-70B-Instruct
  model: "{hf_dir}/models--meta-llama--Llama-3.3-70B-Instruct/snapshots/6f6073b423013f6a7d4d9f39144961bfbfbc386b"
  dtype: bfloat16
  port: 8300
  max-model-len: 65536
  tensor-parallel-size: 2
  # enable-auto-tool-choice: yes
  # tool-call-parser: llama3_json
  # chat-template: "{vllm_dir}/examples/tool_chat_template_llama3.1_json.jinja"
  resources: {cpu: 32, ram: 256, gpu: 2, gpu-ram: 80}

llama70b-4gpu: # confirmed 2025-10-06 11:26:50
  # model: meta-llama/Llama-3.3-70B-Instruct
  model: "{hf_dir}/models--meta-llama--Llama-3.3-70B-Instruct/snapshots/6f6073b423013f6a7d4d9f39144961bfbfbc386b"
  dtype: bfloat16
  port: 8301
  # max-model-len: 65536
  tensor-parallel-size: 4
  # enable-auto-tool-choice: yes
  # tool-call-parser: llama3_json
  # chat-template: "{vllm_dir}/examples/tool_chat_template_llama3.1_json.jinja"
  resources: {cpu: 32, ram: 256, gpu: 4, gpu-ram: 75}

llama70b-8gpu:
  # model: meta-llama/Llama-3.3-70B-Instruct
  model: "{hf_dir}/models--meta-llama--Llama-3.3-70B-Instruct/snapshots/6f6073b423013f6a7d4d9f39144961bfbfbc386b"
  dtype: bfloat16
  port: 8302
  # gpu-memory-utilization: 0.9
  max-model-len: 32768
  max-num-seqs: 8
  tensor-parallel-size: 8
  # enable-auto-tool-choice: yes
  # tool-call-parser: llama3_json
  # chat-template: "{vllm_dir}/examples/tool_chat_template_llama3.1_json.jinja"
  resources: {cpu: 8, ram: 256, gpu: 8, gpu-ram: 20}

llama4-scout: # confirmed 2025-10-06 11:45:55
  # model: meta-llama/Llama-4-Scout-17B-16E-Instruct
  model: "{hf_dir}/models--meta-llama--Llama-4-Scout-17B-16E-Instruct/snapshots/7dab2f5f854fe665b6b2f1eccbd3c48e5f627ad8"
  dtype: bfloat16
  port: 8400
  max-model-len: 65536
  tensor-parallel-size: 4
  # enable-auto-tool-choice: yes
  # tool-call-parser: llama4_json
  # chat-template: "{vllm_dir}/examples/tool_chat_template_llama4_json.jinja"
  # enable-auto-tool-choice: yes
  # tool-call-parser: llama3_json
  # chat-template: "{vllm_dir}/examples/tool_chat_template_llama3.1_json.jinja"
  resources: {cpu: 32, ram: 512, gpu: 4, gpu-ram: 80}


llama4-scout-8gpu: # confirmed 2025-07-25 20:46:21
  # model: meta-llama/Llama-4-Scout-17B-16E-Instruct
  model: "{hf_dir}/models--meta-llama--Llama-4-Scout-17B-16E-Instruct/snapshots/7dab2f5f854fe665b6b2f1eccbd3c48e5f627ad8"
  dtype: bfloat16
  port: 8401
  # max-model-len: 65536
  max-model-len: 65536
  tensor-parallel-size: 8
  # enable-auto-tool-choice: yes
  # tool-call-parser: llama4_json
  # chat-template: "{vllm_dir}/examples/tool_chat_template_llama4_json.jinja"
  # enable-auto-tool-choice: yes
  # tool-call-parser: llama3_json
  # chat-template: "{vllm_dir}/examples/tool_chat_template_llama3.1_json.jinja"
  resources: {cpu: 32, ram: 512, gpu: 8, gpu-ram: 75, gpu-name: H100-80}

llama4-mav: # confirmed 2025-10-06 11:45:55
  # model: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
  model: "{hf_dir}/models--meta-llama--Llama-4-Maverick-17B-128E-Instruct-FP8/snapshots/94125d2bd83076b21eed33119525e29eaf3894f4"
  max-model-len: 400000
  tensor-parallel-size: 8
  port: 8448
  # enable-auto-tool-choice: yes
  # tool-call-parser: llama4_json
  # chat-template: "{vllm_dir}/examples/tool_chat_template_llama4_json.jinja"
  # tool-call-parser: llama4_pythonic
  # chat-template: "{vllm_dir}/examples/tool_chat_template_llama4_pythonic.jinja"
  resources: {cpu: 32, ram: 512, gpu: 8, gpu-ram: 75, gpu-name: H100-80}

llama4-scout-2gpu: # confirmed 2025-10-06 11:45:55
  # model: meta-llama/Llama-4-Scout-17B-16E-Instruct
  model: "{hf_dir}/models--meta-llama--Llama-4-Scout-17B-16E-Instruct/snapshots/7dab2f5f854fe665b6b2f1eccbd3c48e5f627ad8"
  dtype: bfloat16
  port: 8402
  # max-model-len: 65536
  max-model-len: 65536
  tensor-parallel-size: 2
  # enable-auto-tool-choice: yes
  # tool-call-parser: llama4_json
  # chat-template: "{vllm_dir}/examples/tool_chat_template_llama4_json.jinja"
  # enable-auto-tool-choice: yes
  # tool-call-parser: llama3_json
  # chat-template: "{vllm_dir}/examples/tool_chat_template_llama3.1_json.jinja"
  resources: {cpu: 32, ram: 512, gpu: 2, gpu-ram: 150, gpu-name: B200}

llama4-mav-4gpu: # confirmed 2025-10-06 11:38:00
  # model: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
  model: "{hf_dir}/models--meta-llama--Llama-4-Maverick-17B-128E-Instruct-FP8/snapshots/94125d2bd83076b21eed33119525e29eaf3894f4"
  max-model-len: 400000
  tensor-parallel-size: 4
  port: 8444
  resources: {cpu: 32, ram: 512, gpu: 4, gpu-ram: 150, gpu-name: B200}


llama8b: # stale
  model: meta-llama/Llama-3.1-8B-Instruct
  dtype: bfloat16
  port: 8030
  # gpu-memory-utilization: 0.9
  max-model-len: 16384
  # max-num-seqs: 4
  # enable-auto-tool-choice: yes
  # tool-call-parser: llama3_json
  # chat-template: "{vllm_dir}/examples/tool_chat_template_llama3.1_json.jinja"
  resources: {cpu: 16, ram: 128, gpu: 1, gpu-ram: 20}

llama8b-2gpu: # stale
  model: meta-llama/Llama-3.1-8B-Instruct
  dtype: bfloat16
  port: 8031
  # gpu-memory-utilization: 0.9
  max-model-len: 32768
  max-num-seqs: 32
  tensor-parallel-size: 2
  # enable-auto-tool-choice: yes
  # tool-call-parser: llama3_json
  # chat-template: "{vllm_dir}/examples/tool_chat_template_llama3.1_json.jinja"
  resources: {cpu: 4, ram: 128, gpu: 2, gpu-ram: 20}

llama1b: # stale
  model: meta-llama/Llama-3.2-1B-Instruct
  dtype: bfloat16
  port: 8003
  # gpu-memory-utilization: 0.9
  max-model-len: 16384
  max-num-seqs: 1
  # enable-auto-tool-choice: yes
  # tool-call-parser: llama3_json
  # chat-template: "{vllm_dir}/examples/tool_chat_template_llama3.1_json.jinja"
  resources: {cpu: 8, ram: 32, gpu: 1, gpu-ram: 4}

llama1b-large: # stale
  model: meta-llama/Llama-3.2-1B-Instruct
  dtype: bfloat16
  port: 8013
  # gpu-memory-utilization: 0.9
  # enable-auto-tool-choice: yes
  # tool-call-parser: llama3_json
  # chat-template: "{vllm_dir}/examples/tool_chat_template_llama3.1_json.jinja"
  resources: {cpu: 4, ram: 32, gpu: 1, gpu-ram: 20}


gemma3n4: # unconfirmed
  model: google/gemma-3n-E4B-it
  dtype: bfloat16
  port: 8820
  # max-model-len: 16384
  # max-num-seqs: 4
  resources: {cpu: 4, ram: 16, gpu: 1, gpu-ram: 8}

gemma3n2: # unconfirmed
  model: google/gemma-3n-E2B-it
  dtype: bfloat16
  port: 8802
  # max-model-len: 16384
  # max-num-seqs: 4
  resources: {cpu: 4, ram: 16, gpu: 1, gpu-ram: 8}

gemma27b: # confirmed 2025-07-21 14:05:09
  model: google/gemma-3-27b-it
  dtype: bfloat16
  max-model-len: 65536
  port: 8200
  # gpu-memory-utilization: 0.9
  # enable-auto-tool-choice: yes
  # tool-call-parser: pythonic
  # chat-template: "{vllm_dir}/examples/tool_chat_template_gemma3_pythonic.jinja"
  resources: {cpu: 16, ram: 128, gpu: 1, gpu-ram: 75}

gemma4b: # stale
  model: google/gemma-3-4b-it
  dtype: bfloat16
  port: 8022
  # enable-auto-tool-choice: yes
  # tool-call-parser: pythonic
  # chat-template: "{vllm_dir}/examples/tool_chat_template_gemma3_pythonic.jinja"
  resources: {cpu: 8, ram: 64, gpu: 1, gpu-ram: 20}

gemma-tiny: # confirmed 2025-09-20 12:22:41
  model: google/gemma-3-270m-it
  dtype: bfloat16
  port: 8002
  # enable-auto-tool-choice: yes
  # tool-call-parser: pythonic
  # chat-template: "{vllm_dir}/examples/tool_chat_template_gemma3_pythonic.jinja"
  resources: {cpu: 8, ram: 64, gpu: 1, gpu-ram: 20}


r1qwen32b: # unconfirmed
  model: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
  dtype: bfloat16
  port: 8870
  # gpu-memory-utilization: 0.95
  max-model-len: 65536
  max-num-seqs: 1
  resources: {cpu: 16, ram: 128, gpu: 1, gpu-ram: 80}

# --dtype bfloat16 --gpu-memory-utilization 0.95 --max-num-seqs 4 --tensor-parallel-size 2
r1qwen32b-large: # unconfirmed
  model: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
  dtype: bfloat16
  port: 8871
  # gpu-memory-utilization: 0.95
  max-model-len: 65536
  max-num-seqs: 4
  tensor-parallel-size: 2
  resources: {cpu: 16, ram: 128, gpu: 2, gpu-ram: 80}

r1llama70b: # unconfirmed
  model: deepseek-ai/DeepSeek-R1-Distill-Llama-70B
  dtype: bfloat16
  port: 8877
  # gpu-memory-utilization: 0.95
  # max-model-len: 65536
  # max-num-seqs: 1
  tensor-parallel-size: 2
  resources: {cpu: 32, ram: 256, gpu: 2, gpu-ram: 80}


## deprecated (or unused)

# phi4-mm:
#   model: microsoft/Phi-4-multimodal-instruct
#   dtype: bfloat16
#   gpu-memory-utilization: 0.95
#   trust-remote-code: yes
#   enable-auto-tool-choice: yes
#   tool-call-parser: llama3_json
#   chat-template: "{vllm_dir}/examples/tool_chat_template_phi4_mini.jinja"
#   max-model-len: 65536
#   max-num-seqs: 4
#   resources: {cpu: 16, ram: 64, gpu: 1, gpu-ram: 20}

# phi4-mm-large:
#   model: microsoft/Phi-4-multimodal-instruct
#   dtype: bfloat16
#   gpu-memory-utilization: 0.90
#   trust-remote-code: yes
#   resources: {cpu: 8, ram: 64, gpu: 1, gpu-ram: 80}






