
phi4:
#  command: "--dtype bfloat16 --gpu-memory-utilization 0.95 --trust-remote-code --max-model-len 65536 --max-num-seqs 4"
  arguments:
    model: microsoft/Phi-4-multimodal-instruct
    dtype: bfloat16
    gpu-memory-utilization: 0.95
    trust-remote-code: yes
    max-model-len: 65536
    max-num-seqs: 4
  resources: {cpu: 16, ram: 64, gpu: 1, gpu-ram: 20}

qwen-8b:
  model: Qwen/Qwen3-8B
#    command: "--dtype bfloat16 --gpu-memory-utilization 0.95 --max-model-len 16384 --max-num-seqs 1 --enable-reasoning --reasoning-parser deepseek_r1"
  arguments:
    model: Qwen/Qwen3-8B
    dtype: bfloat16
    gpu-memory-utilization: 0.95
    max-model-len: 16384
    max-num-seqs: 1
    enable-reasoning: yes
    reasoning-parser: deepseek_r1
  resources: {cpu: 16, ram: 64, gpu: 1, gpu-ram: 20}

qwen-32b:
  model: Qwen/Qwen3-32B
#  command: "--dtype bfloat16 --gpu-memory-utilization 0.95 --max-num-seqs 1 --max-num-seqs 1 --enable-reasoning --reasoning-parser deepseek_r1"
  arguments:
    model: Qwen/Qwen3-32B
    dtype: bfloat16
    gpu-memory-utilization: 0.95
    max-model-len: 65536
    max-num-seqs: 1
    enable-reasoning: yes
    reasoning-parser: deepseek_r1
  resources: {cpu: 16, ram: 128, gpu: 1, gpu-ram: 80}

mistral-7b:
  model: mistralai/Mistral-7B-Instruct-v0.3
#  command: "--dtype bfloat16 --gpu-memory-utilization 0.95 --max-model-len 16384 --max-num-seqs 1"
  arguments:
    model: mistralai/Mistral-7B-Instruct-v0.3
    dtype: bfloat16
    gpu-memory-utilization: 0.95
    max-model-len: 16384
    max-num-seqs: 1
  resources: {cpu: 16, ram: 64, gpu: 1, gpu-ram: 20}

llama-70b:
  model: meta-llama/Llama-3.3-70B-Instruct
#  command: "--dtype bfloat16 --gpu-memory-utilization 0.95 --max-num-seqs 4 --tensor-parallel-size 2 --enable-auto-tool-choice --tool-call-parser llama3_json --chat-template {vllm_dir}/examples/tool_chat_template_llama3.1_json.jinja"
  arguments:
    model: meta-llama/Llama-3.3-70B-Instruct
    dtype: bfloat16
    gpu-memory-utilization: 0.95
    max-model-len: 65536
    max-num-seqs: 4
    tensor-parallel-size: 2
    enable-auto-tool-choice: yes
    tool-call-parser: llama3_json
    chat-template: "{vllm_dir}/examples/tool_chat_template_llama3.1_json.jinja"
  resources: {cpu: 32, ram: 256, gpu: 2, gpu-ram: 80}

llama-8b:
  model: meta-llama/Llama-3.1-8B-Instruct
  command: "--dtype bfloat16 --gpu-memory-utilization 0.95 --max-model-len 16384 --max-num-seqs 1 --enable-auto-tool-choice --tool-call-parser llama3_json --chat-template {vllm_dir}/examples/tool_chat_template_llama3.1_json.jinja"
  resources: {cpu: 16, ram: 128, gpu: 1, gpu-ram: 20}

llama-1b:
  model: meta-llama/Llama-3.2-1B-Instruct
  command: "--dtype bfloat16 --gpu-memory-utilization 0.95 --max-model-len 16384 --max-num-seqs 1 --enable-auto-tool-choice --tool-call-parser llama3_json --chat-template {vllm_dir}/examples/tool_chat_template_llama3.1_json.jinja"
  resources: {cpu: 8, ram: 32, gpu: 1, gpu-ram: 4}

gemma-27b:
  model: google/gemma-3-27b-it
  command: "--dtype bfloat16 --gpu-memory-utilization 0.95 --max-num-seqs 1"
  resources: {cpu: 16, ram: 128, gpu: 1, gpu-ram: 80}

r1-qwen-32b:
  model: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
  command: "--dtype bfloat16 --gpu-memory-utilization 0.95 --max-model-len 65536 --max-num-seqs 1"
  resources: {cpu: 16, ram: 128, gpu: 1, gpu-ram: 80}

r1-qwen-32b-large:
  model: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
  command: "--dtype bfloat16 --gpu-memory-utilization 0.95 --max-num-seqs 4 --tensor-parallel-size 2"
  resources: {cpu: 16, ram: 128, gpu: 2, gpu-ram: 80}










