

phi4-r:
  model: microsoft/Phi-4-reasoning
  dtype: bfloat16
  gpu-memory-utilization: 0.9
  trust-remote-code: yes
  # max-model-len: 65536
  # max-num-seqs: 4
  resources: {cpu: 16, ram: 64, gpu: 1, gpu-ram: 80}


phi4-mm:
  model: microsoft/Phi-4-multimodal-instruct
  dtype: bfloat16
  gpu-memory-utilization: 0.95
  trust-remote-code: yes
  max-model-len: 65536
  max-num-seqs: 4
  resources: {cpu: 16, ram: 64, gpu: 1, gpu-ram: 20}

phi4-mm-large:
  model: microsoft/Phi-4-multimodal-instruct
  dtype: bfloat16
  gpu-memory-utilization: 0.90
  trust-remote-code: yes
  # max-model-len: 65536
  # max-num-seqs: 16
  resources: {cpu: 8, ram: 64, gpu: 1, gpu-ram: 80}

qwen8b:
  model: Qwen/Qwen3-8B
  dtype: bfloat16
  gpu-memory-utilization: 0.95
  max-model-len: 16384
  max-num-seqs: 1
  enable-reasoning: yes
  reasoning-parser: deepseek_r1
  resources: {cpu: 16, ram: 64, gpu: 1, gpu-ram: 20}

qwen32b:
  # model: Qwen/Qwen3-32B
  model: /fast/fleeb/huggingface_cache/hub/models--Qwen--Qwen3-32B/snapshots/30b8421510892303dc5ddd6cd0ac90ca2053478d
  dtype: bfloat16
  # gpu-memory-utilization: 0.90
  # max-model-len: 65536
  # max-num-seqs: 1
  enable-reasoning: yes
  reasoning-parser: deepseek_r1
  enable-auto-tool-choice: yes
  tool-call-parser: hermes
  resources: {cpu: 16, ram: 128, gpu: 1, gpu-ram: 80}

mistral7b:
  model: mistralai/Mistral-7B-Instruct-v0.3
  dtype: bfloat16
  gpu-memory-utilization: 0.9
  max-model-len: 16384
  max-num-seqs: 1
  resources: {cpu: 16, ram: 64, gpu: 1, gpu-ram: 20}

llama70b:
  model: meta-llama/Llama-3.3-70B-Instruct
  dtype: bfloat16
  gpu-memory-utilization: 0.9
  max-model-len: 65536
  max-num-seqs: 4
  tensor-parallel-size: 2
  enable-auto-tool-choice: yes
  tool-call-parser: llama3_json
  chat-template: "{vllm_dir}/examples/tool_chat_template_llama3.1_json.jinja"
  resources: {cpu: 32, ram: 256, gpu: 2, gpu-ram: 80}

llama70b-8gpu:
  model: meta-llama/Llama-3.3-70B-Instruct
  dtype: bfloat16
  gpu-memory-utilization: 0.9
  max-model-len: 32768
  max-num-seqs: 8
  tensor-parallel-size: 8
  enable-auto-tool-choice: yes
  tool-call-parser: llama3_json
  chat-template: "{vllm_dir}/examples/tool_chat_template_llama3.1_json.jinja"
  resources: {cpu: 8, ram: 256, gpu: 8, gpu-ram: 20}

llama4:
  model: meta-llama/Llama-4-Scout-17B-16E-Instruct
  dtype: bfloat16
  gpu-memory-utilization: 0.9
  # max-num-seqs: 64
  max-model-len: 65536
  tensor-parallel-size: 4
  # enable-auto-tool-choice: yes
  # tool-call-parser: llama3_json
  # chat-template: "{vllm_dir}/examples/tool_chat_template_llama3.1_json.jinja"
  resources: {cpu: 32, ram: 256, gpu: 4, gpu-ram: 80}

# llama4-mav:
#   # model: meta-llama/Llama-4-Maverick-17B-128E-Instruct
#   model: /fast/fleeb/huggingface_cache/hub/models--meta-llama--Llama-4-Maverick-17B-128E-Instruct/snapshots/f0ee6477b90b7a6aaefd2cfdf1b4a05d36184137
#   dtype: bfloat16
#   # gpu-memory-utilization: 0.9
#   # max-num-seqs: 64
#   max-model-len: 430000
#   tensor-parallel-size: 8
#   # enable-auto-tool-choice: yes
#   # tool-call-parser: llama3_json
#   # chat-template: "{vllm_dir}/examples/tool_chat_template_llama3.1_json.jinja"
#   resources: {cpu: 32, ram: 512, gpu: 8, gpu-ram: 75}


llama4-mav-fp8:
  model: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
  # dtype: bfloat16
  # gpu-memory-utilization: 0.9
  # max-num-seqs: 64
  # kv-cache-dtype: fp8
  max-model-len: 400000
  tensor-parallel-size: 8
  port: 8160
  enable-auto-tool-choice: yes
  tool-call-parser: llama4_pythonic
  chat-template: "{vllm_dir}/examples/tool_chat_template_llama4_pythonic.jinja"
  resources: {cpu: 32, ram: 512, gpu: 8, gpu-ram: 75, gpu-name: H100-80}


llama8b:
  model: meta-llama/Llama-3.1-8B-Instruct
  dtype: bfloat16
  gpu-memory-utilization: 0.9
  max-model-len: 16384
  # max-num-seqs: 4
  enable-auto-tool-choice: yes
  tool-call-parser: llama3_json
  chat-template: "{vllm_dir}/examples/tool_chat_template_llama3.1_json.jinja"
  resources: {cpu: 16, ram: 128, gpu: 1, gpu-ram: 20}

llama8b-2gpu:
  model: meta-llama/Llama-3.1-8B-Instruct
  dtype: bfloat16
  gpu-memory-utilization: 0.9
  max-model-len: 32768
  max-num-seqs: 32
  tensor-parallel-size: 2
  enable-auto-tool-choice: yes
  tool-call-parser: llama3_json
  chat-template: "{vllm_dir}/examples/tool_chat_template_llama3.1_json.jinja"
  resources: {cpu: 4, ram: 128, gpu: 2, gpu-ram: 20}

llama1b:
  model: meta-llama/Llama-3.2-1B-Instruct
  dtype: bfloat16
  gpu-memory-utilization: 0.9
  max-model-len: 16384
  max-num-seqs: 1
  enable-auto-tool-choice: yes
  tool-call-parser: llama3_json
  chat-template: "{vllm_dir}/examples/tool_chat_template_llama3.1_json.jinja"
  resources: {cpu: 8, ram: 32, gpu: 1, gpu-ram: 4}

llama1b-large:
  model: meta-llama/Llama-3.2-1B-Instruct
  dtype: bfloat16
  gpu-memory-utilization: 0.9
  enable-auto-tool-choice: yes
  tool-call-parser: llama3_json
  chat-template: "{vllm_dir}/examples/tool_chat_template_llama3.1_json.jinja"
  resources: {cpu: 4, ram: 32, gpu: 1, gpu-ram: 20}

gemma27b:
  model: google/gemma-3-27b-it
  dtype: bfloat16
  # gpu-memory-utilization: 0.90
  # max-model-len: 65536
  # max-num-seqs: 8
  enable-auto-tool-choice: yes
  tool-call-parser: pythonic
  chat-template: "{vllm_dir}/examples/tool_chat_template_gemma3_pythonic.jinja"
  resources: {cpu: 16, ram: 128, gpu: 1, gpu-ram: 75}

gemma4b:
  model: google/gemma-3-4b-it
  dtype: bfloat16
  # gpu-memory-utilization: 0.90
  # max-model-len: 65536
  # max-num-seqs: 8
  enable-auto-tool-choice: yes
  tool-call-parser: pythonic
  chat-template: "{vllm_dir}/examples/tool_chat_template_gemma3_pythonic.jinja"
  resources: {cpu: 8, ram: 64, gpu: 1, gpu-ram: 20}

r1qwen32b:
  model: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
  dtype: bfloat16
  gpu-memory-utilization: 0.95
  max-model-len: 65536
  max-num-seqs: 1
  resources: {cpu: 16, ram: 128, gpu: 1, gpu-ram: 80}

# --dtype bfloat16 --gpu-memory-utilization 0.95 --max-num-seqs 4 --tensor-parallel-size 2
r1qwen32b-large:
  model: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
  dtype: bfloat16
  gpu-memory-utilization: 0.95
  max-model-len: 65536
  max-num-seqs: 4
  tensor-parallel-size: 2
  resources: {cpu: 16, ram: 128, gpu: 2, gpu-ram: 80}










